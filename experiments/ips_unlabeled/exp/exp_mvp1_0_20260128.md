# 🍃 1D Simple System: Trajectory-free Loss Validation
> **Name:** 1D Simple System: Trajectory-free Loss Validation  \
> **ID:** `IPS-20260128-mvp1_0-01`  \
> **Topic:** `ips_unlabeled` | **MVP:** MVP-1.0 | **Project:** `IPS`  \
> **Author:** Viska Wei | **Date:** 2026-01-28 | **Status:** ❌ FAIL
>
> 🎯 **Target:** 验证 trajectory-free loss 能否在简单 1D 系统学习势函数 Φ 和 V  \
> 🚀 **Decision / Next:** Gate-1 FAIL → 检查 loss 推导，解决 identifiability 问题，或尝试 Route B (Kernel方法)

---

## ⚡ 核心结论速览（供 main 提取；≤30行；必含 I/O + Pipeline TL;DR）

> **一句话**: Trajectory-free loss 存在严重的 **identifiability 问题**：Loss 可收敛至 ~0 但 V 误差 98-162%、Φ 误差 19-94%；即使固定已知 V 仅学 Φ，误差仍达 78%。**Gate-1 FAIL**，需重新审视 loss 设计或尝试 Route B。

### 0.1 这实验到底在做什么？（X := 算法/机制 → 目标 | Why+How | I/O | Trade-off）

$$
X := \underbrace{\text{Trajectory-free Weak-form Loss}}_{\text{是什么}}\ \xrightarrow[\text{基于}]{\ \text{弱形式PDE三项 🍎}\ }\ \underbrace{\text{学习势函数 Φ 和 V ⭐}}_{\text{用于}}\ \big|\ \underbrace{\text{Why 🩸}}_{\text{无标签数据无法用轨迹方法}} + \underbrace{\text{How 💧}}_{\text{通过分布演化反演势函数}}
$$
- **🐻 What (是什么)**: 用弱形式 PDE 损失函数，从无标签集合数据学习交互势函数 Φ 和动力学势函数 V
- **🍎 核心机制**: 弱形式三项损失（耗散 J_diss + 扩散 J_diff + 能量变动 J_energy）
- **⭐ 目标**: 验证 trajectory-free 方法能否在简单系统上学习势函数（Gate-1 验证）
- **🩸 Why（痛点）**: 传统方法需要粒子轨迹，但实际只能获得无标签集合快照
- **💧 How（难点）**: 通过分布演化（而非单个粒子轨迹）反演势函数，需要弱形式 PDE 和 AD 计算
$$
\underbrace{\text{I/O 🫐}}_{\text{输入→输出}}\ =\ \underbrace{\Delta^+}_{\text{无需轨迹信息 🍀}}\ -\ \underbrace{\Delta^-}_{\text{需要更多时间快照/计算复杂度 👿}}
$$
**I/O（必须写清楚，读者靠这一段理解实验"在干嘛"）**

| 类型 | 符号 | 说明 | 示例 |
|------|------|------|------|
| **🫐 输入** | $\mathcal{D}$ | 无标签集合数据 $\{X_{t_\ell}^{(m)}\}$ | shape: (M, L, N, d) = (200, 20, 10, 1) |
| **🫐 输入** | $\theta$ | NN 参数（学习 Φ 和 V） | MLP [d, 64, 64, 1] |
| **🫐 输出** | $\hat{\Phi}, \hat{V}$ | 学到的势函数 | 函数形式（NN 表示） |
| **📊 指标** | Relative L² error | $\|\hat{\Phi} - \Phi_{true}\|_2 / \|\Phi_{true}\|_2$ | 目标 < 10% |
| **🍁 基线** | 真实势函数 | $\Phi_{true}(r) = e^{-r^2}$, $V_{true}(x) = 0.5x^2$ | Oracle |
| **🍀 指标Δ** | 学习误差 | 相对 L² 误差 | 决定是否继续 Route A |

### 0.2 Pipeline TL;DR（5-10 行极简伪代码，一眼看懂在跑什么）

> ⚠️ **关键**：核心循环必须写清楚每步的**输出是什么**，用具体数据结构示例说明

```
1. 准备数据：SDE 模拟器生成（1D, N=5, L=10, M=30, σ=0.1）
2. 定义真实势函数：V_true(x)=0.5x², Φ_true(r)=exp(-r²) (Gaussian)
3. 构建模型：MLP [1, 64, 64, 1] for Φ 和 V（分开两个网络）
4. 核心循环：
   for epoch in 1..100:
       for (t_ℓ, t_{ℓ+1}) in time_pairs:
           J_diss = Σ|∇V + ∇Φ*μ|² / N * Δt    # 耗散项
           J_diff = σ² * Σ(ΔV + ΔΦ*μ) / N * Δt  # 扩散项
           J_energy = E(t_{ℓ+1}) - E(t_ℓ)        # 能量变化
           residual = J_diss + J_diff - 2*J_energy
           loss = residual²                       # 平方确保非负
           loss.backward() → optimizer.step()
       → 单步输出: {'epoch': 31, 'loss': 1e-7, 'V_err': 162%, 'Φ_err': 19%}
       → ⚠️ 问题：loss→0 但误差仍很高！
5. 评估：V 误差 98-162%，Φ 误差 19-94%（全部 FAIL）
6. 落盘：results/mvp1_0/*.json + img/mvp1_0_*.png
```

> ⚠️ **实验发现**: Loss 收敛到 ~1e-7 但势函数完全错误！存在 **identifiability 问题**。

> ⚠️ **复现命令**（repo/entry/config/seed）→ 见 §7.2 附录
> 📖 **详细流程树状图**（完整可视化）→ 见 §2.4.1
> 📖 **详细伪代码**（对齐真实代码）→ 见 §2.4.2

### 0.3 对假设/验证问题的回答

| 验证问题 | 结果 | 结论 |
|---------|------|------|
| Q1: Trajectory-free loss 是否有效？ | ❌ **FAIL** | Loss→0 但势函数误差>90%，存在 identifiability 问题 |
| Q1.1: 弱形式PDE推导是否正确？ | ⚠️ **存疑** | Loss 可收敛但不能保证正确解，需重新审视推导 |
| Q1.2: 离散化误差是否可控？ | ✅ 可控 | 收敛表现良好，问题不在离散化 |
| Q2.2: AD计算梯度/Laplacian是否稳定？ | ✅ 稳定 | 梯度计算正常，无数值爆炸 |
| **新发现**: V-Φ identifiability | ❌ **严重** | V 和 Φ 可相互补偿，loss=0 时二者均可完全错误 |

### 0.4 关键数字（只放最重要的 3-5 个）

| Metric | Value | vs Baseline | Notes |
|--------|-------|------------|------|
| Relative L² error (Φ) | **19.30% - 94.10%** | > 10% ❌ FAIL | 多配置均失败 |
| Relative L² error (V) | **98.72% - 162.31%** | > 10% ❌ FAIL | 学到的 V 方向完全错误 |
| Training loss (final) | **~1e-7** | - | Loss 收敛但势函数错误！ |
| Φ-only error (known V) | **78.13%** | > 10% ❌ FAIL | 即使固定 V 也无法学 Φ |
| Training epochs | 31-32 (early stop) | - | 很快收敛到错误解 |

### 0.5 Links

| Type | Link |
|------|------|
| 🧠 Hub | `experiments/ips_unlabeled/ips_unlabeled_hub.md` § Q1, Gate-1 |
| 🗺️ Roadmap | `experiments/ips_unlabeled/ips_unlabeled_roadmap.md` § MVP-1.0 |
| 📋 Session | `experiments/ips_unlabeled/sessions/1.md` |

---

# 1. 🎯 目标

**核心问题**: Trajectory-free loss 能否在简单 1D 系统学习势函数 Φ 和 V？

**对应 main / roadmap**:
- 验证问题：Q1 (Trajectory-free loss 有效性)
- 子假设：Q1.1 (弱形式PDE推导), Q1.2 (离散化误差), Q2.2 (AD稳定性)
- Gate：Gate-1 (Loss 有效性验证)

## 1.1 成功标准（验收 / stop rule）

| 场景 | 预期结果 | 判断标准 |
|------|---------|---------|
| ✅ 通过 | Relative L² error < 10% on both Φ and V | If 误差<10% → 继续 Route A，扩展到多粒子/高维 |
| ❌ 否决 | Relative L² error ≥ 10% | If 误差≥10% → 检查 loss 推导，或尝试 Route B (Kernel方法) |
| ⚠️ 异常 | Loss 不收敛 / 梯度爆炸 | 先检查：AD 计算、数值稳定性、学习率 |

---

# 2. 🦾 方法（算法 + I/O + 实验流程）

## 2.1 算法

### 2.1.1 核心算法

**Trajectory-free Weak-form Loss**：

$$
L(\Phi, V) = \sum_{\ell=1}^{L-1} \left[ J_{diss}(\ell) + J_{diff}(\ell) - 2 J_{energy}(\ell) \right]
$$

其中三项分别为：

**耗散项**（Dissipation）：
$$
J_{diss}(\ell) = \frac{1}{M} \sum_{m=1}^M \frac{1}{N} \sum_{i=1}^N \left| \nabla V(X_{t_\ell}^{(m,i)}) + \frac{1}{N}\sum_{j=1}^N \nabla\Phi(X_{t_\ell}^{(m,i)} - X_{t_\ell}^{(m,j)}) \right|^2 \cdot \Delta t_\ell
$$

**扩散项**（Diffusion）：
$$
J_{diff}(\ell) = \frac{\sigma}{M} \sum_{m=1}^M \frac{1}{N^2} \sum_{i,j=1}^N \left[ \Delta V(X_{t_\ell}^{(m,i)}) + \Delta\Phi(X_{t_\ell}^{(m,i)} - X_{t_\ell}^{(m,j)}) \right] \cdot \Delta t_\ell
$$

**能量变动项**（Energy change）：
$$
J_{energy}(\ell) = -\frac{2}{M} \sum_{m=1}^M \frac{1}{N^2} \sum_{i,j=1}^N \left[ V(X_{t_{\ell+1}}^{(m,i)}) + \Phi(X_{t_{\ell+1}}^{(m,i)} - X_{t_{\ell+1}}^{(m,j)}) - V(X_{t_\ell}^{(m,i)}) - \Phi(X_{t_\ell}^{(m,i)} - X_{t_\ell}^{(m,j)}) \right]
$$

**直觉解释**：
- **耗散项**：捕捉粒子间相互作用（斥力/引力）导致的能量耗散
- **扩散项**：捕捉随机热运动（布朗运动）的扩散效应
- **能量变动项**：捕捉分布整体能量水平的变化

### 2.1.2 符号表

| 符号 | 含义 | 类型/取值范围 | 计算/来源 | 具体数值例子 |
|------|------|--------------|-----------|-------------|
| $M$ | 独立样本数 | int, $M > 0$ | 数据配置 | `M=200` |
| $L$ | 时间快照数 | int, $L > 1$ | 数据配置 | `L=20` |
| $N$ | 粒子数 | int, $N > 0$ | 数据配置 | `N=10` |
| $d$ | 空间维度 | int, $d=1$ | 实验设定 | `d=1` |
| $X_{t_\ell}^{(m,i)}$ | 第 m 个样本、时刻 t_ℓ、第 i 个粒子的位置 | $\mathbb{R}^d$ | 数据输入 | `X[0,5,3] = 0.42` (样本0, 时刻5, 粒子3) |
| $\Phi(r)$ | 交互势函数 | $\mathbb{R} \to \mathbb{R}$ | NN 学习 | `Φ(0.5) = 0.78` |
| $V(x)$ | 动力学势函数 | $\mathbb{R} \to \mathbb{R}$ | NN 学习 | `V(0.3) = 0.045` |
| $\nabla\Phi$ | Φ 的梯度 | $\mathbb{R}^d \to \mathbb{R}^d$ | AD 计算 | `∇Φ(0.5) = -1.56` |
| $\Delta\Phi$ | Φ 的 Laplacian | $\mathbb{R}^d \to \mathbb{R}$ | AD 计算 | `ΔΦ(0.5) = 3.12` |
| $\sigma$ | 噪声强度 | float, $\sigma > 0$ | 数据配置 | `σ=0.1` |
| $\Delta t_\ell$ | 时间间隔 | float, $\Delta t_\ell > 0$ | 数据配置 | `Δt = 0.1` |

### 2.1.3 辅助公式

**对称化约束**（物理约束）：
$$
\Phi(x) = \frac{1}{2}\left[\tilde{\Phi}(x) + \tilde{\Phi}(-x)\right]
$$

- **用途**: 确保交互势函数满足对称性（物理合理性）
- **输入**: 原始网络输出 $\tilde{\Phi}(x)$
- **输出**: 对称化后的 $\Phi(x)$，用于损失计算

**自动微分（AD）计算导数**：
- **用途**: 计算 $\nabla V$, $\nabla\Phi$, $\Delta V$, $\Delta\Phi$，避免数值微分噪声
- **输入**: 网络输出 $V(x)$, $\Phi(r)$
- **输出**: 通过 PyTorch autograd 计算的高阶导数

## 2.2 输入 / 输出（必填：详细展开，事无巨细）

### I/O Schema

| Component | Type/Shape | Example | Notes |
|----------|------------|---------|------|
| Input: 数据 $\mathcal{D}$ | numpy array | shape=(M, L, N, d) = (200, 20, 10, 1) | 无标签集合数据 |
| Input: 时间点 $t_\ell$ | numpy array | shape=(L,) = (20,) | 时间快照时刻 |
| Input: 真实势函数（仅用于评估） | function | `phi_true(r) = np.exp(-r**2)`, `v_true(x) = 0.5*x**2` | Oracle，不用于训练 |
| Output: 学到的 $\hat{\Phi}$ | PyTorch Module | `MLP(input_dim=1, hidden=[64,64], output_dim=1)` | 对称化后使用 |
| Output: 学到的 $\hat{V}$ | PyTorch Module | `MLP(input_dim=1, hidden=[64,64], output_dim=1)` | 或与 Φ 共享网络 |
| Output: 训练指标 | dict | `{'loss': 0.123, 'phi_err': 0.08, 'v_err': 0.05}` | 每个 epoch 记录 |

### Assumptions & Constraints

| Assumption/Constraint | Why it matters | How handled |
|----------------------|----------------|------------|
| 数据来自 SDE 模拟 | 确保数据符合物理模型 | 使用 MVP-0.0 验证过的生成器 |
| 时间快照连续 | 能量项需要配对 $(t_\ell, t_{\ell+1})$ | 确保数据包含连续时间点 |
| 势函数平滑 | 保证 AD 计算稳定 | 使用平滑激活函数（如 tanh/ReLU） |
| 对称性约束 | 物理合理性 | 实现对称化层 |

## 2.3 实现要点（详细展开，读者能对照代码定位）

| What | Where (file:function) | Key detail |
|------|------------------------|-----------|
| config parsing | `scripts/train_nn.py:parse_args` | 命令行参数：M, L, N, d, 网络结构, 训练参数 |
| data generation | `core/sde_simulator.py:SDESimulator.simulate` | Euler-Maruyama 模拟，返回 (M, L, N, d) 数组 |
| model: Φ network | `core/nn_models.py:SymmetricMLP` | 对称化 MLP：Φ(x) = ½(Φ̃(x) + Φ̃(-x)) |
| model: V network | `core/nn_models.py:MLP` | 标准 MLP [d, 64, 64, 1] |
| model: combined | `core/nn_models.py:PotentialNetworks` | 封装 V 和 Φ，提供 grad/laplacian 方法 |
| loss: 三项计算 | `core/trajectory_free_loss.py:TrajectoryFreeLoss.forward` | ⚠️ **问题可能在这里** |
| loss: J_diss | `core/trajectory_free_loss.py:compute_drift` | AD 计算 ∇V 和 ∇Φ |
| loss: J_diff | `core/trajectory_free_loss.py:compute_laplacian_sum` | AD 计算 ΔV 和 ΔΦ |
| loss: J_energy | `core/trajectory_free_loss.py:compute_energy` | 势能求和 |
| training loop | `scripts/train_nn.py:train` | Adam + early stopping |
| evaluator | `core/true_potentials.py:evaluate_V_error, evaluate_Phi_error` | 计算相对 L² 误差 |
| visualizer | `scripts/train_nn.py:plot_potential_comparison` | 真势 vs 学得势对比图 |

## 2.4 实验流程（必填：树状可视化 + 模块拆解 + 核心循环展开 + Code Pointer）

### 2.4.1 实验流程树状图（完整可视化）

```
实验流程
│
├── 1. 准备数据
│   ├── 数据源：MVP-0.0 生成的数据，或重新生成
│   ├── 配置：1D, N=10, L=20, M=200, σ=0.1
│   ├── 真实势函数：V_true(x)=0.5x², Φ_true(r)=exp(-r²)
│   └── 输出: data = array(M, L, N, d) = (200, 20, 10, 1)
│
├── 2. 构建模型
│   ├── Φ network: MLP [1, 64, 64, 1] + 对称化层
│   ├── V network: MLP [1, 64, 64, 1]
│   └── 输出: model = {'phi': InteractionPotential(), 'v': ExternalPotential()}
│
├── 3. 核心循环 ⭐
│   ├── 外层：for epoch in 1..1000
│   ├── 中层：for batch in time_pairs (t_ℓ, t_{ℓ+1})
│   └── 内层：计算损失 → 反向传播 → 更新参数
│       ├── Step 1: 计算 J_diss (使用 AD 计算 ∇V, ∇Φ)
│       ├── Step 2: 计算 J_diff (使用 AD 计算 ΔV, ΔΦ)
│       ├── Step 3: 计算 J_energy (时间配对计算能量差)
│       ├── Step 4: loss = J_diss + J_diff - 2*J_energy
│       ├── Step 5: loss.backward() → optimizer.step()
│       └── Step 6: 记录 → record = {'epoch': e, 'loss': 0.123, 'phi_err': 0.15, 'v_err': 0.08}
│   └── 循环后输出: metrics = [record_1, record_2, ...] (共 1000 条)
│
├── 4. 评估
│   ├── 输入: trained_model + data
│   ├── 计算：相对 L² 误差 on Φ 和 V
│   ├── 可视化：真势 vs 学得势对比图
│   └── 输出: summary = {'phi_l2_err': 0.08, 'v_l2_err': 0.05, 'figures': [...]}
│
└── 5. 落盘
    ├── 模型文件：results/mvp1_0/model.pth
    ├── 指标文件：results/mvp1_0/metrics.json
    ├── 图表文件：img/phi_comparison.png, img/v_comparison.png, img/loss_curve.png
    └── 输出示例: {"phi_l2_err": 0.08, "v_l2_err": 0.05, ...}
```

### 2.4.2 模块拆解（详细展开每个模块，带 Code Pointer）

| Module | Responsibility | Input → Output | Code Pointer |
|--------|----------------|----------------|--------------|
| M1: load_config | 读取配置 | cli → cfg | `scripts/train_nn.py:parse_args` |
| M2: load_data | 读取/生成数据 | cfg → data | `scripts/generate_data.py:generate` |
| M3: build_model | 构建 NN 模型 | cfg → model | `core/potentials.py:build_potentials` |
| M4: train_loop | **核心循环** | model + data → trained_model + metrics | `scripts/train_nn.py:train` |
| M5: compute_loss | 计算弱形式损失 | model + data → loss | `core/potentials.py:compute_weak_form_loss` |
| M6: evaluate | 计算指标 | trained_model + data → metrics | `core/validation.py:compute_l2_error` |
| M7: visualize | 可视化对比 | trained_model + data → figures | `scripts/train_nn.py:plot_comparison` |
| M8: save | 落盘 | metrics + model → files | `utils/io.py:save_results` |

### 2.4.3 核心循环展开（对齐真实代码的详细伪代码）

```python
# === 核心循环（对齐 scripts/train_nn.py:train）===

def train_trajectory_free(model, data, cfg):
    """
    输入:
        model: {'phi': InteractionPotential(), 'v': ExternalPotential()}
        data: array(M, L, N, d) = (200, 20, 10, 1)
        cfg: 配置 {epochs: 1000, lr: 0.001, ...}
    
    输出:
        trained_model: 训练好的模型
        metrics: list of dict, 每条记录格式如下:
            {'epoch': 100, 'loss': 0.123, 'phi_err': 0.15, 'v_err': 0.08}
    """
    optimizer = torch.optim.Adam(
        list(model['phi'].parameters()) + list(model['v'].parameters()),
        lr=cfg.lr
    )
    metrics = []
    
    for epoch in range(cfg.epochs):
        epoch_loss = 0.0
        
        # 遍历时间配对 (t_ℓ, t_{ℓ+1})
        for ell in range(cfg.L - 1):
            # Step 1: 获取时间配对数据
            X_t = data[:, ell, :, :]      # shape: (M, N, d)
            X_tp1 = data[:, ell+1, :, :]   # shape: (M, N, d)
            dt = cfg.time_points[ell+1] - cfg.time_points[ell]
            
            # Step 2: 计算耗散项 J_diss
            # 对每个样本 m 和粒子 i
            J_diss = 0.0
            for m in range(cfg.M):
                for i in range(cfg.N):
                    x_i = X_t[m, i, :]  # shape: (d,)
                    
                    # 计算 ∇V(x_i)
                    v_i = model['v'](x_i)
                    grad_v = torch.autograd.grad(v_i, x_i, create_graph=True)[0]
                    
                    # 计算 (1/N)Σ_j ∇Φ(x_i - x_j)
                    grad_phi_sum = 0.0
                    for j in range(cfg.N):
                        x_j = X_t[m, j, :]
                        r_ij = x_i - x_j
                        phi_ij = model['phi'](r_ij)
                        grad_phi_ij = torch.autograd.grad(phi_ij, r_ij, create_graph=True)[0]
                        grad_phi_sum += grad_phi_ij
                    grad_phi_mean = grad_phi_sum / cfg.N
                    
                    # |∇V + ∇Φ*μ|²
                    force = grad_v + grad_phi_mean
                    J_diss += torch.norm(force)**2
            
            J_diss = J_diss / (cfg.M * cfg.N) * dt
            
            # Step 3: 计算扩散项 J_diff (类似，使用 Laplacian)
            J_diff = compute_diffusion(model, X_t, cfg, dt)
            
            # Step 4: 计算能量变动项 J_energy
            J_energy = compute_energy_change(model, X_t, X_tp1, cfg)
            
            # Step 5: 总损失
            loss = J_diss + J_diff - 2 * J_energy
            epoch_loss += loss.item()
            
            # Step 6: 反向传播
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        # Step 7: 评估（每 100 epochs）
        if epoch % 100 == 0:
            phi_err, v_err = evaluate_l2_error(model, data, cfg)
            record = {
                'epoch': epoch,
                'loss': epoch_loss / (cfg.L - 1),
                'phi_err': phi_err,
                'v_err': v_err
            }
            metrics.append(record)
    
    return model, metrics
    # 返回: trained_model + [{'epoch':0, 'loss':0.5, ...}, {'epoch':100, 'loss':0.2, ...}, ...]
```

### 2.4.4 参数扫描（如果有 sweep，必须详细展开）

本实验暂不进行参数扫描，固定配置验证核心方法。

### 2.4.5 复现清单（必须完整填写）

- [ ] 固定随机性：seed=42 (数据生成 + 模型初始化)
- [ ] 固定数据版本：使用 MVP-0.0 生成的数据，或固定生成参数
- [ ] 固定对照组：真实势函数 Φ_true, V_true (Oracle)
- [ ] 输出物：`results/mvp1_0/model.pth` + `results/mvp1_0/metrics.json` + `img/*.png` + `logs/mvp1_0.log`

---

# 3. 🧪 实验设计（具体到本次实验）

## 3.1 数据 / 环境

| Item | Value |
|------|-------|
| Source | 实时生成（使用 `SDESimulator`） |
| Shape | (M, L, N, d) = (30, 10, 5, 1) |
| Split | 全部用于训练（无验证集，因为这是验证性实验） |
| Feature | d=1, 粒子位置 $X \in \mathbb{R}$ |
| Target | 真实势函数 $\Phi_{true}(r) = e^{-r^2}$, $V_{true}(x) = 0.5x^2$ (仅用于评估) |
| SDE 参数 | σ=0.1, dt=0.01, T=1.0, V_k=1.0, Φ_A=1.0, Φ_σ=1.0 |

## 3.2 Baselines（对照组）

| Baseline | Purpose | Key config |
|----------|---------|-----------|
| Oracle (真实势函数) | 上界 | $\Phi_{true}$, $V_{true}$ |
| Random NN (未训练) | 下界 | 随机初始化的 NN |

## 3.3 训练 / 运行配置

| Param | Value | Notes |
|------|-------|------|
| epochs | 100 | 实际 31-32 epochs (early stopping) |
| batch | 全量数据 | 不进行 batch，使用所有时间配对 |
| lr | 0.001 - 0.01 | 尝试多个值，结果类似 |
| optimizer | Adam | 默认参数 |
| patience | 20-30 | Early stopping patience |
| hardware | CPU | 单核运行 |
| time per epoch | ~1.5s | 小配置下 |

## 3.4 扫描参数（可选）

本实验暂不进行参数扫描。

## 3.5 评价指标

| Metric | Definition | Why |
|--------|------------|-----|
| Relative L² error (Φ) | $\|\hat{\Phi} - \Phi_{true}\|_2 / \|\Phi_{true}\|_2$ | 主要指标，决定 Gate-1 是否通过 |
| Relative L² error (V) | $\|\hat{V} - V_{true}\|_2 / \|V_{true}\|_2$ | 次要指标 |
| Training loss | $L(\Phi, V)$ | 收敛性指标 |
| Gradient norm | $\|\nabla_\theta L\|$ | 数值稳定性检查 |

---

# 4. 📊 图表 & 结果

> ⚠️ 图表文字必须全英文！

### Fig 1: Training History (Loss & Components)
![](./img/mvp1_0_training_history.png)

**What it shows**: Training loss and loss components (J_diss, J_diff, J_energy) vs epochs

**Key observations**:
- Loss 快速收敛至 ~1e-7，但这并不意味着学到了正确的势函数
- J_diss（耗散项）占主导，J_diff 和 J_energy 相对较小
- Early stopping 在 epoch 31-32 触发

---

### Fig 2: Potential Comparison (V and Φ)
![](./img/mvp1_0_potential_comparison.png)

**What it shows**: True V vs Learned V (左); True Φ vs Learned Φ (右)

**Key observations**:
- **V 完全错误**: 真实 V 是凸函数 (V=0.5x²)，但学到的 V 是凹函数（方向完全反了）
- **Φ 形状错误**: 真实 Φ 是单调递减的 Gaussian，但学到的 Φ 先增后减
- V 误差 98-162%，Φ 误差 19-94%

---

### Fig 3: Φ-only Experiment (with known V)
![](./img/mvp1_0b_phi_comparison.png)

**What it shows**: 固定 V 为真实值时，仅学习 Φ 的结果

**Key observations**:
- 即使 V 已知，Φ 学习仍然失败（误差 78.13%）
- Loss 没有收敛到 0（停在 0.018）
- 说明问题不仅是 V-Φ trade-off，loss 公式本身可能有问题

---

# 5. 💡 洞见（解释"为什么会这样"）

## 5.1 机制层（Mechanism)

> ⚠️ **必填**：详细解释为什么会得到这样的结果，从机制层面深入分析。

### 5.1.1 Identifiability 问题的数学本质

弱形式损失 $L = J_{diss} + J_{diff} - 2J_{energy}$ 理论上应等于 0（由能量耗散等式推导），但存在以下问题：

**V-Φ 可交换性**: 在 loss 公式中，V 和 Φ 的梯度项以 $\nabla V + \frac{1}{N}\sum_j \nabla\Phi$ 形式出现。当 N 较小时：
- 如果 $V$ 学成 $V_{true} + c$ 的形式（c 为常数）
- $\Phi$ 可以学成 $\Phi_{true} - c$ 来补偿
- 两者的梯度项相互抵消，loss 仍为 0

**常数偏移不敏感**: Loss 只依赖势函数的梯度和 Laplacian，对常数偏移不敏感：
- $V(x) + C$ 和 $V(x)$ 给出相同的 $\nabla V$
- 这意味着势函数只能学到"差一个常数"的程度

**更深层问题**: 即使固定 V 为真值，Φ 仍学不对（78% 误差），说明：
- 弱形式离散化可能丢失了关键信息
- 或者当前 loss 公式本身有推导错误

### 5.1.2 为什么 Loss→0 但势函数完全错误？

实验观察到 Loss ≈ 1e-7 但 V 误差 162%，原因可能是：

1. **平凡解问题**: 存在使 loss=0 的平凡解家族
2. **局部最优陷阱**: 网络收敛到错误的局部最优
3. **Loss 公式有 Bug**: $J_{diss} + J_{diff} - 2J_{energy}$ 的系数可能有误

## 5.2 实验层（Diagnostics)

> ⚠️ **必填**：详细诊断实验结果，排除所有可能的 confounder。

### 5.2.1 排除的 Confounders

| Confounder | 检查方法 | 结论 |
|------------|---------|------|
| 数据生成错误 | MVP-0.0 已验证 OU 过程 | ✅ 排除 |
| 随机种子依赖 | seed=42 固定 | ✅ 排除 |
| 网络容量不足 | 尝试 [32,32] 和 [64,64] | ✅ 排除（两者都失败） |
| 学习率不合适 | 尝试 0.01, 0.005, 0.001 | ✅ 排除（都收敛到错误解） |
| 数据量不足 | 尝试 M=30, 100, 200 | ✅ 排除（都失败） |
| AD 计算错误 | 梯度正常，无 NaN | ✅ 排除 |

### 5.2.2 根因分析

**最可能的原因**: Loss 公式存在理论问题
- 弱形式 PDE 的离散化可能丢失了势函数的唯一性约束
- 需要额外的正则化项（如势函数归一化）来恢复 identifiability

## 5.3 设计层（So what)

> ⚠️ **必填**：详细阐述对系统/产品/研究路线的影响和启示。

### 5.3.1 对 Route A 的影响

**当前 Route A (Trajectory-free Loss) 无法直接使用**：
- Gate-1 验证失败
- 需要重新审视 loss 公式推导
- 可能需要添加正则化项或约束

### 5.3.2 可能的修复方向

1. **添加 identifiability 约束**:
   - 固定 $V(0) = 0$ 或 $\Phi(0) = 0$
   - 添加 $\int V dx = 0$ 归一化约束

2. **检查 loss 公式系数**:
   - $J_{diss} + J_{diff} - 2J_{energy}$ 中的系数 -2 是否正确？
   - $\sigma^2$ 项是否正确处理？

3. **考虑 Route B (Kernel 方法)**:
   - 直接估计分布演化
   - 可能天然具有更好的 identifiability

### 5.3.3 设计原则教训

> **教训**: 在复杂 loss 设计时，必须验证 loss=0 是否对应唯一正确解。收敛性 ≠ 正确性。

---

# 6. 📝 结论 & 下一步

## 6.1 核心发现（punch line）
> **Trajectory-free loss 存在致命的 identifiability 问题：Loss 收敛到 1e-7 但 V 误差 162%、Φ 误差 94%。Gate-1 验证失败。**

- ❌ Q1: Trajectory-free loss **无效** — 当前形式无法学习正确的势函数
- **Decision**: **拒绝当前方案**，需重新审视 loss 公式或转向 Route B

## 6.2 关键结论（详细展开，不限制条数）

| # | 结论 | 证据（图/表/数字） | 适用范围 |
|---|------|-------------------|---------|
| 1 | Loss=0 不保证正确解 | Loss~1e-7, V err=162%, Φ err=94% | 所有无约束弱形式 loss |
| 2 | V-Φ 存在 identifiability 问题 | 不同配置下误差分布不同但都失败 | N 较小时更严重 |
| 3 | 即使固定 V，Φ 仍学不对 | Φ-only 误差 78%，loss=0.018 | 说明问题不仅是 V-Φ trade-off |
| 4 | AD 计算本身是正确的 | 梯度正常，无数值问题 | 可继续用于其他方法 |
| 5 | 数据生成是正确的 | MVP-0.0 已验证 | SDE 模拟器可复用 |

## 6.3 Trade-offs（Δ+ vs Δ-）

| Upside (Δ+) | Cost / Constraint (Δ-) | When acceptable |
|-------------|--------------------------|----------------|
| 不需要轨迹数据 | 存在 identifiability 问题，当前形式无法使用 | 需先解决 identifiability |
| 计算效率高（单步 loss） | Loss=0 不意味着正确，需要额外约束 | 添加正则化后 |
| AD 计算稳定 | 弱形式离散化可能丢失信息 | 需验证离散化精度 |

## 6.4 下一步（可执行任务）

| Priority | Task | Owner | Link |
|----------|------|-------|------|
| 🔴 P0 | 重新推导 loss 公式，检查系数是否正确 | - | 理论分析 |
| 🔴 P0 | 添加 identifiability 约束（如 V(0)=0, Φ(∞)=0） | - | MVP-1.1 |
| 🟡 P1 | 验证 loss 公式中 σ² 系数是否正确 | - | 理论分析 |
| 🟡 P1 | 尝试 Route B (Kernel 方法) 作为替代 | - | MVP-2.0 |
| 🟢 P2 | 调研文献中类似问题的解决方案 | - | 文献综述 |

---

# 7. 📎 附录（复现/审计用）

## 7.1 数值结果（全量）

### 配置 1: Joint V+Φ (small network)
| Config | V Error | Φ Error | Final Loss | Epochs | Notes |
|--------|---------|---------|------------|--------|-------|
| N=5, L=10, M=30, hidden=[32,32], lr=0.01 | **162.31%** | **19.30%** | ~0 | 31 (early stop) | V 方向完全错误 |

### 配置 2: Joint V+Φ (larger network)
| Config | V Error | Φ Error | Final Loss | Epochs | Notes |
|--------|---------|---------|------------|--------|-------|
| N=5, L=10, M=30, hidden=[64,64], lr=0.001 | **98.72%** | **94.10%** | 1.3e-7 | 32 (early stop) | 两者都错误 |

### 配置 3: Φ-only (known V)
| Config | Φ Error | Final Loss | Epochs | Notes |
|--------|---------|------------|--------|-------|
| N=5, L=10, M=30, hidden=[64,64], lr=0.01 | **78.13%** | 0.018 | 100 | 即使 V 已知也失败 |

## 7.2 执行记录（复现命令）

| Item | Value |
|------|-------|
| Repo | `~/ips_unlabeled_learning` |
| Script | `scripts/train_nn.py` (Joint V+Φ), `scripts/train_phi_only.py` (Φ-only) |
| Seed | 42 |
| Output | `results/mvp1_0/`, `results/mvp1_0_phi_only/` |

```bash
# (1) setup
cd ~/ips_unlabeled_learning
source init.sh

# (2) train joint V+Φ (配置 1)
python scripts/train_nn.py --N 5 --L 10 --M 30 --hidden_dims 32,32 \
    --lr 0.01 --epochs 100 --patience 20 --seed 42

# (3) train joint V+Φ (配置 2)
python scripts/train_nn.py --N 5 --L 10 --M 30 --hidden_dims 64,64 \
    --lr 0.001 --epochs 100 --patience 20 --seed 42

# (4) train Φ-only with known V
python scripts/train_phi_only.py --N 5 --L 10 --M 30 \
    --lr 0.01 --epochs 100 --patience 30 --seed 42
```

## 7.3 运行日志摘要 / Debug

| Issue | Root cause | Fix |
|------|------------|-----|
| Loss 为负 | 原始 loss = J_diss + J_diff - 2*J_energy 可为负 | 改为 loss = residual² |
| 训练输出不实时 | stdout 缓冲 | 添加 `flush=True`, `PYTHONUNBUFFERED=1` |
| V 方向完全反了 | Identifiability 问题 | **未解决** — 需要理论分析 |
| Φ 形状错误 | 同上 | **未解决** |

## 7.4 关键代码位置

| 模块 | 文件 | 函数/类 | 说明 |
|------|------|---------|------|
| 数据生成 | `core/sde_simulator.py` | `SDESimulator.simulate()` | Euler-Maruyama 方法 |
| 势函数 NN | `core/nn_models.py` | `PotentialNetworks`, `SymmetricMLP` | V 和 Φ 网络 |
| Loss 函数 | `core/trajectory_free_loss.py` | `TrajectoryFreeLoss.forward()` | ⚠️ 问题可能在这里 |
| 评估 | `core/true_potentials.py` | `evaluate_V_error()`, `evaluate_Phi_error()` | 计算 L² 误差 |

---

> **实验完成时间**: 2026-01-28 (多次运行，均失败)
