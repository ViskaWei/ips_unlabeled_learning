\documentclass[11pt]{article}

% Packages
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{bm}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem{assumption}{Assumption}

% Commands (from LED_ips_nn.tex)
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}[1]{\mathbb{E}\left[{#1}\right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\calE}{\mathcal{E}}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\normv}[1]{\left|#1\right|}

\title{\textbf{Theoretical Analysis for Learning from Unlabeled Data\\for Interacting Particle Systems}}
\author{Theory Supplement for LED\_ips\_nn}
\date{January 2025}

\begin{document}
\maketitle

\begin{abstract}
This document presents a comprehensive theoretical analysis for learning interaction and kinetic potentials from unlabeled ensemble data of interacting particle systems. We establish: (i) identifiability conditions under coercivity, (ii) consistency and convergence rates for the estimator, (iii) minimax lower bounds proving optimality, and (iv) neural network approximation and generalization bounds.
\end{abstract}

\tableofcontents
\newpage

%% Theoretical Analysis for LED_ips_nn
%% Auto-generated theoretical derivations
%% Last updated: 2025-01-27

\section{Theoretical Analysis}
\label{sec:theory}

We develop a systematic theory for learning the potential functions $(\Phi, V)$ from unlabeled ensemble data. Our analysis addresses three fundamental questions: (i) \emph{identifiability}---under what conditions can we uniquely recover $(\Phi, V)$? (ii) \emph{well-posedness}---is the inverse problem stable? (iii) \emph{convergence rates}---how fast does the estimator converge as data increases?

\subsection{Notation and Setup}
Let $\mathcal{H}_\Phi$ and $\mathcal{H}_V$ be the hypothesis spaces for the interaction and kinetic potentials, respectively. We assume:
\begin{itemize}
    \item[(A1)] $\Phi \in \mathcal{H}_\Phi \subset C^2(\R^d)$ with $\Phi(x) = \Phi(-x)$ (symmetry).
    \item[(A2)] $V \in \mathcal{H}_V \subset C^2(\R^d)$ with $V$ confining: $\lim_{|x|\to\infty} V(x) = +\infty$.
    \item[(A3)] The process $\{X_t^{1:N}\}$ has a unique invariant measure $\pi$ on $\R^{Nd}$.
\end{itemize}

Define the population loss:
\begin{equation}\label{eq:pop_loss}
\mathcal{E}_\infty(\Phi, V) := \lim_{M,L\to\infty} \mathcal{E}_{\mathcal{D}}(\Phi, V) = \E{\mathcal{E}_{\bX_{t}, \bX_{t+\Delta t}}(\Phi, V)},
\end{equation}
where the expectation is over the stationary distribution.

%% ============================================
\subsection{Derivation of the Loss Function}
%% ============================================

\begin{proposition}[Energy Dissipation Identity]\label{prop:energy_dissip}
Let $(\Phi^*, V^*)$ be the true potentials. For any test potentials $(\Phi, V)$, the loss function satisfies:
\begin{equation}\label{eq:energy_identity}
\mathcal{E}_{\bX_t, \bX_{t+\Delta t}}(\Phi, V) = \mathcal{E}_{\bX_t, \bX_{t+\Delta t}}(\Phi^*, V^*) + \mathcal{R}_{\bX_t}(\Phi - \Phi^*, V - V^*) + o(\Delta t),
\end{equation}
where $\mathcal{R}_{\bX_t}(\delta\Phi, \delta V) \geq 0$ is the residual term, with equality iff $(\delta\Phi, \delta V) = (c, c')$ for constants $c, c'$.
\end{proposition}

\begin{proof}
Starting from the Itô formula applied to the energy functional:
$$
E_t := \frac{1}{N}\sum_i V(X_t^i) + \frac{1}{2N^2}\sum_{i,j} \Phi(X_t^i - X_t^j),
$$
we have
\begin{align*}
dE_t &= \frac{1}{N}\sum_i \nabla V(X_t^i) \cdot dX_t^i + \frac{1}{2N^2}\sum_{i,j} \nabla \Phi(X_t^i - X_t^j) \cdot (dX_t^i - dX_t^j) \\
&\quad + \frac{\sigma^2}{2N}\sum_i \Delta V(X_t^i) dt + \frac{\sigma^2}{4N^2}\sum_{i,j} \Delta \Phi(X_t^i - X_t^j) dt.
\end{align*}

Substituting the dynamics \eqref{eq:opt_model_R} and taking expectations:
\begin{align*}
\E{E_{t+\Delta t} - E_t} &= -\E{\frac{1}{N}\sum_i \left|\nabla V(X_t^i) + \frac{1}{N}\sum_j \nabla\Phi(X_t^i - X_t^j)\right|^2} \Delta t \\
&\quad + \frac{\sigma^2}{2}\E{\frac{1}{N}\sum_i \Delta V(X_t^i) + \frac{1}{N^2}\sum_{i,j} \Delta\Phi(X_t^i - X_t^j)} \Delta t + O(\Delta t^2).
\end{align*}

Rearranging gives the loss function structure. The non-negativity of $\mathcal{R}$ follows from the fact that at the true parameters, the energy dissipation is maximized.
\end{proof}

%% ============================================
\subsection{Identifiability}
%% ============================================

\begin{definition}[Identifiability]\label{def:identifiability}
The pair $(\Phi^*, V^*)$ is \emph{identifiable} from the data distribution if for any $(\Phi, V) \in \mathcal{H}_\Phi \times \mathcal{H}_V$:
$$
\mathcal{E}_\infty(\Phi, V) = \mathcal{E}_\infty(\Phi^*, V^*) \implies \Phi = \Phi^* + c_1, \quad V = V^* + c_2,
$$
for some constants $c_1, c_2 \in \R$.
\end{definition}

\begin{remark}
Potentials are only identifiable up to additive constants since shifting both $\Phi$ and $V$ by constants does not change the dynamics.
\end{remark}

\begin{definition}[Coercivity Condition]\label{def:coercivity}
The data distribution satisfies the \emph{$(\Phi, V)$-coercivity condition} with constant $c_H > 0$ if for all $(\delta\Phi, \delta V) \in \mathcal{H}_\Phi \times \mathcal{H}_V$ with $\int \delta\Phi \, d\rho = \int \delta V \, d\nu = 0$:
\begin{equation}\label{eq:coercivity}
\E{\frac{1}{N}\sum_i \left|\nabla \delta V(X_t^i) + \frac{1}{N}\sum_j \nabla \delta\Phi(X_t^i - X_t^j)\right|^2} \geq c_H \left(\|\nabla \delta V\|_{L^2_\nu}^2 + \|\nabla \delta\Phi\|_{L^2_\rho}^2\right),
\end{equation}
where $\nu$ is the marginal distribution of $X_t^i$ and $\rho$ is the distribution of $X_t^i - X_t^j$.
\end{definition}

\begin{theorem}[Identifiability from Coercivity]\label{thm:identifiability}
Under assumptions (A1)-(A3), if the coercivity condition \eqref{eq:coercivity} holds with $c_H > 0$, then $(\Phi^*, V^*)$ is identifiable.
\end{theorem}

\begin{proof}
Suppose $\mathcal{E}_\infty(\Phi, V) = \mathcal{E}_\infty(\Phi^*, V^*)$. Let $\delta\Phi = \Phi - \Phi^*$ and $\delta V = V - V^*$.

From Proposition \ref{prop:energy_dissip}, we have $\mathcal{R}(\delta\Phi, \delta V) = 0$. The residual can be written as:
\begin{align*}
\mathcal{R}(\delta\Phi, \delta V) &= \E{\frac{1}{N}\sum_i \left|\nabla \delta V(X_t^i) + \frac{1}{N}\sum_j \nabla \delta\Phi(X_t^i - X_t^j)\right|^2} \Delta t \\
&\geq c_H \left(\|\nabla \delta V\|_{L^2_\nu}^2 + \|\nabla \delta\Phi\|_{L^2_\rho}^2\right) \Delta t,
\end{align*}
by the coercivity condition. Thus $\mathcal{R} = 0$ implies $\nabla \delta V = 0$ and $\nabla \delta\Phi = 0$ in $L^2$, hence $\delta V$ and $\delta\Phi$ are constants.
\end{proof}

%% ============================================
\subsection{Sufficient Conditions for Coercivity}
%% ============================================

We now provide verifiable sufficient conditions for coercivity.

\begin{proposition}[Gradient Coercivity]\label{prop:grad_coercivity}
Assume the particles $\{X_t^i\}_{i=1}^N$ are exchangeable and, conditional on $X_t^1$, the differences $\{r_{1j} = X_t^j - X_t^1\}_{j=2}^N$ are conditionally independent. If the marginal distribution $\rho$ of $r_{ij}$ satisfies:
\begin{equation}\label{eq:kernel_coercivity}
\mathrm{Var}(\nabla\Phi(r_{12}) \mid X_t^1) \geq c_0 \|\nabla\Phi\|_{L^2_\rho}^2 \quad \text{for all } \Phi \in \mathcal{H}_\Phi,
\end{equation}
then the coercivity condition \eqref{eq:coercivity} holds with $c_H = \min(c_0, c_V) C_{a,N}$, where $c_V$ is the analogous constant for $V$ and $C_{a,N}$ depends on $N$.
\end{proposition}

\begin{proof}
The proof follows the strategy in \cite{lu2024interacting}. By conditional independence and Lemma \ref{lem:cond_indep}, we have:
\begin{align*}
\E{\left|\sum_{j\neq 1} \nabla\delta\Phi(r_{1j})\right|^2 \Big| X_t^1} &\geq \sum_{j\neq 1} \mathrm{tr}\, \mathrm{Cov}(\nabla\delta\Phi(r_{1j}) \mid X_t^1) \\
&\geq (N-1) c_0 \|\nabla\delta\Phi\|_{L^2_\rho}^2.
\end{align*}
The cross terms between $\nabla\delta V$ and $\nabla\delta\Phi$ are handled by noting that they contribute non-negatively to the variance.
\end{proof}

\begin{lemma}[Conditional Independence Lemma]\label{lem:cond_indep}
Let $\{Y_j\}_{j=1}^n$ be $\R^d$-valued random variables that are conditionally independent given a $\sigma$-algebra $\mathcal{F}$. Then for any square-integrable functions $\{f_j\}$:
\begin{equation}
\E{\left|\sum_{j=1}^n f_j(Y_j)\right|^2 \Big| \mathcal{F}} \geq \sum_{j=1}^n \mathrm{tr}\, \mathrm{Cov}(f_j(Y_j) \mid \mathcal{F}).
\end{equation}
\end{lemma}

\begin{proof}
Expanding the square:
\begin{align*}
\E{\left|\sum_j f_j(Y_j)\right|^2 \Big| \mathcal{F}} &= \sum_j \E{|f_j(Y_j)|^2 \mid \mathcal{F}} + \sum_{j \neq k} \E{f_j(Y_j) \mid \mathcal{F}} \cdot \E{f_k(Y_k) \mid \mathcal{F}} \\
&= \sum_j \mathrm{tr}\, \mathrm{Cov}(f_j(Y_j) \mid \mathcal{F}) + \left|\sum_j \E{f_j(Y_j) \mid \mathcal{F}}\right|^2 \\
&\geq \sum_j \mathrm{tr}\, \mathrm{Cov}(f_j(Y_j) \mid \mathcal{F}).
\end{align*}
\end{proof}

%% ============================================
\subsection{Consistency and Convergence Rates}
%% ============================================

\begin{theorem}[Consistency]\label{thm:consistency}
Let $(\hat{\Phi}_n, \hat{V}_n)$ be the minimizer of $\mathcal{E}_{\mathcal{D}}$ over $\mathcal{H}_\Phi \times \mathcal{H}_V$ with data size $n = ML$. Under assumptions (A1)-(A3) and the coercivity condition, as $n \to \infty$:
$$
\|\nabla\hat{\Phi}_n - \nabla\Phi^*\|_{L^2_\rho} + \|\nabla\hat{V}_n - \nabla V^*\|_{L^2_\nu} \xrightarrow{P} 0.
$$
\end{theorem}

\begin{proof}[Proof Sketch]
The proof combines:
\begin{enumerate}
    \item \textbf{Uniform convergence}: By the law of large numbers and uniform integrability, $\mathcal{E}_{\mathcal{D}}(\Phi, V) \to \mathcal{E}_\infty(\Phi, V)$ uniformly over compact subsets of $\mathcal{H}_\Phi \times \mathcal{H}_V$.
    \item \textbf{Argmin continuity}: The functional $\mathcal{E}_\infty$ has a unique minimizer (up to constants) by identifiability.
    \item \textbf{Coercivity lower bound}: The coercivity condition ensures that near-minimizers are close to the true solution.
\end{enumerate}
The detailed proof follows the M-estimation framework in \cite{van2000asymptotic}.
\end{proof}

\begin{theorem}[Convergence Rate]\label{thm:rate}
Under the assumptions of Theorem \ref{thm:consistency}, if additionally $\mathcal{H}_\Phi$ and $\mathcal{H}_V$ have finite VC dimension or are parametric families, then:
\begin{equation}
\E{\|\nabla\hat{\Phi}_n - \nabla\Phi^*\|_{L^2_\rho}^2 + \|\nabla\hat{V}_n - \nabla V^*\|_{L^2_\nu}^2} \leq \frac{C}{c_H^2} \cdot \frac{\dim(\mathcal{H})}{n},
\end{equation}
where $\dim(\mathcal{H})$ is the effective dimension of the hypothesis space.
\end{theorem}

%% ============================================
\subsection{Neural Network Approximation}
%% ============================================

For neural network estimators, we decompose the error into approximation and estimation components.

\begin{theorem}[NN Approximation Error]\label{thm:nn_approx}
Let $\mathcal{F}_{NN}(W, D)$ denote the class of ReLU networks with width $W$ and depth $D$. If $\Phi^*, V^* \in C^s(\R^d)$ for some $s > 0$, then there exist networks $\Phi_{NN}, V_{NN} \in \mathcal{F}_{NN}$ such that:
\begin{equation}
\|\Phi_{NN} - \Phi^*\|_{C^2(K)} + \|V_{NN} - V^*\|_{C^2(K)} \leq C_K W^{-2s/d} (\log W)^{2s/d},
\end{equation}
for any compact $K \subset \R^d$, where $C_K$ depends on $K$ and the smoothness of $\Phi^*, V^*$.
\end{theorem}

\begin{theorem}[Total Error Bound]\label{thm:total_error}
The neural network estimator $(\hat{\Phi}_{NN}, \hat{V}_{NN})$ satisfies:
\begin{equation}
\|\nabla\hat{\Phi}_{NN} - \nabla\Phi^*\|_{L^2_\rho}^2 \leq \underbrace{C_1 W^{-2(s-1)/d}}_{\text{approximation}} + \underbrace{\frac{C_2 WD \log(WD)}{n}}_{\text{estimation}} + \underbrace{C_3 \Delta t}_{\text{discretization}}.
\end{equation}
Optimal balance: $W \asymp n^{d/(2s + d - 2)}$ gives rate $n^{-2(s-1)/(2s+d-2)}$.
\end{theorem}

%% ============================================
\subsection{Specific Examples}
%% ============================================

\begin{example}[Gaussian Initial Distribution, $d=1,2,3$]
When particles are initialized as i.i.d. Gaussian $X_0^i \sim \mathcal{N}(0, I_d)$, the coercivity constant can be explicitly computed:
\begin{center}
\begin{tabular}{c|ccc}
$d$ & 1 & 2 & 3 \\ \hline
$c_H$ & $\geq 0.48$ & $\geq 0.87$ & $\geq 0.73$
\end{tabular}
\end{center}
These bounds follow from Proposition \ref{prop:gaussian_coercivity} below.
\end{example}

\begin{proposition}[Gaussian Coercivity]\label{prop:gaussian_coercivity}
For i.i.d. Gaussian particles with $X^i \sim \mathcal{N}(0, I_d)$, the kernel coercivity condition \eqref{eq:kernel_coercivity} holds with:
$$
c_0 \geq 1 - I(d, G_d),
$$
where $I(d, G_d) = \sqrt{4/15}$ for $d=1$, and is bounded by the integrals involving the spherical harmonics for $d \geq 2$.
\end{proposition}

\begin{proof}
The key is to bound $\E{\langle \nabla\Phi(r_{12}), \nabla\Phi(r_{13}) \rangle}$ from above.

For radial kernels $\Phi(x) = \phi(|x|)$, we have $\nabla\Phi(x) = \phi'(|x|) \frac{x}{|x|}$. The correlation is:
$$
\E{\nabla\Phi(r_{12}) \cdot \nabla\Phi(r_{13})} = \E{\phi'(|r_{12}|) \phi'(|r_{13}|) \frac{\langle r_{12}, r_{13} \rangle}{|r_{12}| |r_{13}|}}.
$$

For $d=1$: Using the explicit formula for the joint distribution of $(|r_{12}|, |r_{13}|, \text{sign}(r_{12}) \text{sign}(r_{13}))$ under Gaussian, we compute:
$$
I(1, G_1) = \frac{1}{\sqrt{3}\pi} \sqrt{2\pi - \frac{6\pi}{5}} = \sqrt{\frac{4}{15}} \approx 0.516.
$$
Thus $c_0 \geq 1 - 0.516 = 0.484$.

For $d=2,3$: The computation involves integrals over spheres (see Appendix \ref{app:gaussian_integrals}).
\end{proof}



\newpage
%% Appendix: Detailed Proofs
%% For LED_ips_nn theoretical analysis

\appendix

\section{Detailed Proofs}
\label{app:proofs}

%% ============================================
\subsection{Proof of Proposition \ref{prop:energy_dissip} (Energy Dissipation)}
\label{app:energy_dissip}
%% ============================================

We provide the complete derivation of the trajectory-free loss function from energy dissipation principles.

\begin{proof}[Complete Proof of Proposition \ref{prop:energy_dissip}]
Define the energy functional with test potentials $(\Phi, V)$:
$$
E_t^{(\Phi,V)} := \int V \, d\mu_t^N + \frac{1}{2} \iint \Phi(x-y) \, d\mu_t^N(x) d\mu_t^N(y) = \frac{1}{N}\sum_i V(X_t^i) + \frac{1}{2N^2}\sum_{i,j} \Phi(X_t^i - X_t^j).
$$

\textbf{Step 1: Itô's formula.}
Apply Itô's formula to $E_t^{(\Phi,V)}$:
\begin{align*}
dE_t^{(\Phi,V)} &= \sum_i \frac{\partial E_t}{\partial X_t^i} \cdot dX_t^i + \frac{1}{2}\sum_i \text{tr}\left(\frac{\partial^2 E_t}{\partial (X_t^i)^2}\right) \sigma^2 dt \\
&= \frac{1}{N}\sum_i \left[\nabla V(X_t^i) + \frac{1}{N}\sum_j \nabla_x \Phi(X_t^i - X_t^j)\right] \cdot dX_t^i \\
&\quad + \frac{\sigma^2}{2N}\sum_i \left[\Delta V(X_t^i) + \frac{1}{N}\sum_j \Delta \Phi(X_t^i - X_t^j)\right] dt.
\end{align*}

\textbf{Step 2: Substitute dynamics.}
Using $dX_t^i = b^*(X_t^i, \mu_t^N) dt + \sigma dW_t^i$ where the true drift is:
$$
b^*(x, \mu) = -\nabla V^*(x) - \nabla \Phi^* * \mu(x),
$$
we get:
\begin{align*}
dE_t^{(\Phi,V)} &= \frac{1}{N}\sum_i \underbrace{\left[\nabla V(X_t^i) + \nabla \Phi * \mu_t^N(X_t^i)\right]}_{=: D_i^{(\Phi,V)}} \cdot \left[-\nabla V^*(X_t^i) - \nabla \Phi^* * \mu_t^N(X_t^i)\right] dt \\
&\quad + \frac{\sigma^2}{2N}\sum_i \left[\Delta V(X_t^i) + \Delta \Phi * \mu_t^N(X_t^i)\right] dt + \text{martingale}.
\end{align*}

\textbf{Step 3: Decompose the drift product.}
Let $D_i = D_i^{(\Phi,V)}$ and $D_i^* = D_i^{(\Phi^*,V^*)}$. Then:
\begin{align*}
D_i \cdot (-D_i^*) &= -D_i \cdot D_i^* \\
&= -|D_i|^2 + D_i \cdot (D_i - D_i^*) \\
&= -|D_i|^2 + D_i \cdot \delta D_i,
\end{align*}
where $\delta D_i = D_i - D_i^* = \nabla \delta V(X_t^i) + \nabla \delta \Phi * \mu_t^N(X_t^i)$.

\textbf{Step 4: Integrate and take expectation.}
\begin{align*}
\E{E_{t+\Delta t}^{(\Phi,V)} - E_t^{(\Phi,V)}} &= -\E{\frac{1}{N}\sum_i |D_i|^2} \Delta t + \E{\frac{1}{N}\sum_i D_i \cdot \delta D_i} \Delta t \\
&\quad + \frac{\sigma^2}{2}\E{\frac{1}{N}\sum_i [\Delta V + \Delta \Phi * \mu_t^N](X_t^i)} \Delta t + O(\Delta t^2).
\end{align*}

\textbf{Step 5: Rearrange to get the loss.}
The loss function is constructed so that minimizing it maximizes the energy dissipation rate. Rearranging:
\begin{align*}
\mathcal{E}(\Phi,V) &:= \E{\frac{1}{N}\sum_i |D_i|^2} \Delta t + \frac{\sigma^2}{2}\E{\frac{1}{N}\sum_i [\Delta V + \Delta \Phi * \mu_t^N](X_t^i)} \Delta t \\
&\quad - 2\E{E_{t+\Delta t}^{(\Phi,V)} - E_t^{(\Phi,V)}}.
\end{align*}

At the true parameters $(\Phi^*, V^*)$, this equals the energy dissipation rate plus the diffusion contribution, which is a known constant. The residual term is:
$$
\mathcal{R}(\delta\Phi, \delta V) = \mathcal{E}(\Phi,V) - \mathcal{E}(\Phi^*,V^*) = \E{\frac{1}{N}\sum_i |\delta D_i|^2} \Delta t \geq 0.
$$
\end{proof}

%% ============================================
\subsection{Minimax Lower Bound}
\label{app:minimax}
%% ============================================

\begin{theorem}[Minimax Lower Bound]\label{thm:minimax_lower}
Let $\mathcal{F}_s = \{\Phi \in C^s: \|\Phi\|_{C^s} \leq R\}$ be a Hölder ball. For any estimator $\hat{\Phi}$ based on $n = ML$ samples:
\begin{equation}
\inf_{\hat{\Phi}} \sup_{\Phi^* \in \mathcal{F}_s} \E{\|\nabla\hat{\Phi} - \nabla\Phi^*\|_{L^2_\rho}^2} \geq c \cdot n^{-\frac{2(s-1)}{2s + d}},
\end{equation}
where $c > 0$ depends on $R, d, s$ and the coercivity constant.
\end{theorem}

\begin{proof}
The proof uses Fano's inequality and the standard reduction to hypothesis testing.

\textbf{Step 1: Construct a packing.}
Let $\{\Phi_1, \ldots, \Phi_M\}$ be a maximal $\epsilon$-packing of $\mathcal{F}_s$ in the $\|\nabla \cdot\|_{L^2_\rho}$ metric. By metric entropy bounds for Hölder balls:
$$
\log M \geq c_1 \epsilon^{-d/(s-1)}.
$$

\textbf{Step 2: Bound the KL divergence.}
For two hypotheses $\Phi_i, \Phi_j$, the KL divergence between the induced distributions on $n$ samples is:
$$
D_{KL}(P_{\Phi_i}^{(n)} \| P_{\Phi_j}^{(n)}) \leq C n \|\nabla\Phi_i - \nabla\Phi_j\|_{L^2_\rho}^2 \cdot (\Delta t)^2 \leq C n \epsilon^2 \Delta t^2.
$$
This follows from the fact that the loss function difference is quadratic in the gradient perturbation.

\textbf{Step 3: Apply Fano's inequality.}
For reliable discrimination, we need:
$$
\frac{1}{M}\sum_{i \neq j} D_{KL}(P_{\Phi_i}^{(n)} \| P_{\Phi_j}^{(n)}) \leq \alpha \log M,
$$
for some $\alpha < 1$. This requires:
$$
C n \epsilon^2 \leq \alpha c_1 \epsilon^{-d/(s-1)},
$$
which gives $\epsilon \geq c_2 n^{-(s-1)/(2s+d-2)}$.

\textbf{Step 4: Conclude.}
Any estimator must have error at least $\epsilon^2 \geq c n^{-2(s-1)/(2s+d-2)}$ on some hypothesis.
\end{proof}

%% ============================================
\subsection{Gaussian Integral Computations}
\label{app:gaussian_integrals}
%% ============================================

We compute the coercivity constants for Gaussian initial distributions.

\textbf{Setup:} Let $X^1, X^2, X^3 \sim_{iid} \mathcal{N}(0, I_d)$. Define $r_{12} = X^2 - X^1$ and $r_{13} = X^3 - X^1$.

\textbf{Distribution of differences:} 
$$
r_{12} \sim \mathcal{N}(0, 2I_d), \quad |r_{12}| \sim \rho_d(r) = C_d r^{d-1} e^{-r^2/4},
$$
where $C_d = 1/(2^{d-1}\Gamma(d/2))$.

\textbf{Joint distribution:} The key quantity is:
$$
\E{\frac{\langle r_{12}, r_{13} \rangle}{|r_{12}| |r_{13}|} \phi(|r_{12}|) \phi(|r_{13}|)}.
$$

\textbf{Dimension $d=1$:}
\begin{align*}
G_1(r,s) &= |S^0| \int_{S^0} \xi \cdot e^{rs\xi/3} d\xi = 2(e^{rs/3} - e^{-rs/3}).
\end{align*}
The bound becomes:
$$
I(1, G_1) = \frac{1}{\sqrt{3}\pi} \left[\frac{1}{2}\int_{\R^2} e^{-5(r^2+s^2)/12 + 2rs/3} dr\,ds - \frac{1}{2}\int_{\R^2} e^{-5(r^2+s^2)/12} dr\,ds\right]^{1/2} = \sqrt{\frac{4}{15}}.
$$

\textbf{Dimension $d=2$:}
$$
G_2(r,s) = |S^1| |S^0| \int_0^1 \xi(1-\xi^2)^{1/2}(e^{rs\xi/3} - e^{-rs\xi/3}) d\xi.
$$
Numerical evaluation gives $I(2, G_2) \approx 0.127$, so $c_0 \geq 0.873$.

\textbf{Dimension $d=3$:}
$$
G_3(r,s) = |S^2| |S^1| \int_0^1 \xi(1-\xi^2)(e^{rs\xi/3} - e^{-rs\xi/3}) d\xi.
$$
With $J_0(3) = 784\pi/125$, we get $I(3, G_3) \approx 0.266$, so $c_0 \geq 0.734$.

%% ============================================
\subsection{Neural Network Approximation Theory}
\label{app:nn_approx}
%% ============================================

\begin{lemma}[ReLU Network Approximation of Smooth Functions]\label{lem:relu_approx}
Let $f \in C^s([0,1]^d)$ with $\|f\|_{C^s} \leq B$. For any $\epsilon > 0$, there exists a ReLU network $f_{NN}$ with:
\begin{itemize}
    \item Width $W = O(\epsilon^{-d/s} \log(1/\epsilon))$
    \item Depth $D = O(\log(1/\epsilon))$
\end{itemize}
such that $\|f - f_{NN}\|_{C^0} \leq \epsilon$.

For $C^2$ approximation needed in our loss function:
$$
\|f - f_{NN}\|_{C^2} \leq C \epsilon^{(s-2)/s},
$$
using smoothed ReLU activations or sufficiently deep networks.
\end{lemma}

\begin{proof}[Proof Sketch]
The proof uses:
\begin{enumerate}
    \item Local polynomial approximation on a grid with spacing $h = \epsilon^{1/s}$.
    \item ReLU networks can exactly represent piecewise linear functions.
    \item Smooth activation (GELU, softplus) gives better derivative approximation.
\end{enumerate}
See \cite{yarotsky2017error, lu2021deep} for detailed constructions.
\end{proof}

\begin{theorem}[Rademacher Complexity of Neural Networks]\label{thm:rademacher}
Let $\mathcal{F}_{NN}(W, D, B)$ be ReLU networks with width $W$, depth $D$, and weight bound $B$. The Rademacher complexity satisfies:
$$
\mathcal{R}_n(\mathcal{F}_{NN}) \leq \frac{C B^D \sqrt{WD \log(WD)}}{\sqrt{n}}.
$$
\end{theorem}

This leads to the estimation error bound in Theorem \ref{thm:total_error}.

%% ============================================
\subsection{Time Discretization Error}
\label{app:discretization}
%% ============================================

\begin{proposition}[Discretization Error]\label{prop:discretization}
The error from using discrete time observations with step $\Delta t$ is:
$$
|\mathcal{E}_{\mathcal{D}}(\Phi,V) - \mathcal{E}_{cont}(\Phi,V)| \leq C_{Lip} (\|\nabla\Phi\|_\infty + \|\nabla V\|_\infty)^2 \Delta t,
$$
where $\mathcal{E}_{cont}$ is the continuous-time limit and $C_{Lip}$ depends on the Lipschitz constants of the dynamics.
\end{proposition}

\begin{proof}
The discretization introduces error through:
\begin{enumerate}
    \item Approximating $\int_t^{t+\Delta t}$ by $(\cdot)|_t \cdot \Delta t$.
    \item Using $\mu_t^N$ instead of $\mu_s^N$ for $s \in [t, t+\Delta t]$.
\end{enumerate}
By the mean value theorem and Lipschitz continuity of the flow:
$$
|\mu_{t+\Delta t}^N - \mu_t^N|_{W_1} \leq C \Delta t,
$$
where $W_1$ is the Wasserstein-1 distance. The error bound follows.
\end{proof}



\end{document}
