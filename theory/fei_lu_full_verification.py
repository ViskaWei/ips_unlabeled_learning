"""
Complete Verification of Fei Lu et al. Theory
============================================

Based on:
- "Learning Interaction Kernels in Stochastic Systems of Interacting Particles 
   from Multiple Trajectories" (Foundations of Computational Mathematics, 2021)
- "On the coercivity condition in the learning of interacting particle systems"
   (Li & Lu, arXiv:2011.10480)

This script verifies each theoretical claim step-by-step.
"""

import numpy as np
from scipy import integrate
from scipy.stats import norm
import sympy as sp
from sympy import symbols, exp, sqrt, pi, integrate as sym_integrate, oo, simplify

print("=" * 70)
print("FEI LU THEORY VERIFICATION")
print("=" * 70)

# ============================================================
# SECTION 1: SYSTEM SETUP
# ============================================================
print("\n" + "=" * 70)
print("SECTION 1: SYSTEM SETUP")
print("=" * 70)

print("""
The system (Equation 1.1 in the paper):

  dx_{i,t} = (1/N) Î£_j Ï†(||x_j - x_i||)(x_j - x_i) dt + Ïƒ dB_{i,t}

Key definitions:
- Ï†: R+ â†’ R is the interaction kernel
- Î¦'(r) = Ï†(r)r is the derivative of the pairwise potential
- Ï_T: measure of pairwise distances on [0,T]

The function space norm (Equation 1.6):
  |||Ï†||| = ||Ï†(Â·)Â·||_{LÂ²(Ï_T)} = (âˆ« |Ï†(r)r|Â² Ï_T(dr))^{1/2}
  
Note: We learn Î¦'(r) = Ï†(r)r, not Ï†(r) directly!
""")

# ============================================================
# SECTION 2: COERCIVITY CONDITION
# ============================================================
print("\n" + "=" * 70)
print("SECTION 2: COERCIVITY CONDITION")
print("=" * 70)

print("""
From Section 3.1 of Fei Lu's paper, the coercivity condition is:

DEFINITION (Coercivity): The hypothesis space H satisfies the coercivity 
condition with constant c_H > 0 if for all Ï† âˆˆ H:

  E[ ||f_Ï†(X)||Â² ] â‰¥ c_H Â· |||Ï†|||Â²

where f_Ï† is the drift field induced by Ï†.

EQUIVALENT FORM: Let G_T be the integral operator
  (G_T Ï†)(r) = E[ Î£_{jâ‰ 1} Ï†(r_{1j}) Â· (r_{1j}/|r_{1j}|) | r_{12} = r ]

Then coercivity is equivalent to:
  ||G_T Ï†||_{LÂ²(Ï_T)} â‰¥ c_H Â· ||Ï†||_{LÂ²(Ï_T)}

This is about the positive definiteness of an integral kernel!
""")

# ============================================================
# SECTION 3: NUMERICAL VERIFICATION OF COERCIVITY
# ============================================================
print("\n" + "=" * 70)
print("SECTION 3: NUMERICAL VERIFICATION")
print("=" * 70)

def compute_coercivity_fei_lu(d, N, n_samples=100000):
    """
    Compute the coercivity constant following Fei Lu's definition.
    
    The key is: we're computing
      c_H = inf_Ï† E[||f_Ï†(X)||Â²] / |||Ï†|||Â²
    
    For N particles with positions X_i ~ N(0,I_d) independently at t=0:
    
    f_Ï†(X)_1 = (1/N) Î£_{jâ‰ 1} Ï†(|r_{1j}|) r_{1j}
    
    where r_{1j} = X_j - X_1.
    """
    np.random.seed(42)
    
    # Generate N independent particles
    X = np.random.randn(n_samples, N, d)
    
    # Compute pairwise differences from particle 1
    # r_{1j} = X_j - X_1 for j = 2, ..., N
    r = X[:, 1:, :] - X[:, 0:1, :]  # shape: (n_samples, N-1, d)
    
    # Compute norms |r_{1j}|
    r_norm = np.linalg.norm(r, axis=2, keepdims=True)  # (n_samples, N-1, 1)
    
    # Unit vectors r_{1j} / |r_{1j}|
    r_unit = r / (r_norm + 1e-10)  # (n_samples, N-1, d)
    
    # For Ï†(r) = 1 (constant kernel):
    # f_Ï†(X)_1 = (1/N) Î£_{jâ‰ 1} r_{1j}
    
    # Actually, for Fei Lu's setup:
    # f_Ï†(X)_i = (1/N) Î£_j Ï†(|r_{ij}|) r_{ij}
    # where Ï†(r)r = Î¦'(r)
    
    # So for Ï†(r) = 1/r (giving Î¦'(r) = 1):
    # f_Ï†(X)_1 = (1/N) Î£_{jâ‰ 1} r_{1j}/|r_{1j}| = (1/N) Î£_{jâ‰ 1} u_{1j}
    
    # Compute f_Ï† for Ï†(r) = 1/r
    f_phi = np.mean(r_unit, axis=1)  # (n_samples, d), averaged over j
    
    # E[||f_Ï†||Â²]
    E_f_sq = np.mean(np.sum(f_phi**2, axis=1))
    
    # |||Ï†|||Â² = E[|Ï†(r)r|Â²] = E[1Â²] = 1 for Ï†(r)r = 1
    phi_norm_sq = 1.0
    
    # Coercivity constant
    c_H = E_f_sq / phi_norm_sq
    
    print(f"d={d}, N={N}:")
    print(f"  E[||f_Ï†||Â²] = {E_f_sq:.6f}")
    print(f"  |||Ï†|||Â² = {phi_norm_sq:.6f}")
    print(f"  c_H = E[||f_Ï†||Â²] / |||Ï†|||Â² = {c_H:.6f}")
    
    # For comparison, compute with different N
    return c_H

print("\n--- Coercivity with varying N (at t=0, iid Gaussian) ---")
for N in [2, 3, 5, 10, 20]:
    compute_coercivity_fei_lu(d=1, N=N, n_samples=200000)
    print()

# ============================================================
# SECTION 4: THE MEAN-FIELD LIMIT
# ============================================================
print("\n" + "=" * 70)
print("SECTION 4: MEAN-FIELD LIMIT (N â†’ âˆ)")
print("=" * 70)

print("""
In the mean-field limit N â†’ âˆ, the empirical measure Î¼_N converges to 
a deterministic measure Î¼. The drift becomes:

  f_Ï†(x) = âˆ« Ï†(|y-x|)(y-x) Î¼(dy)

For Gaussian Î¼ = N(0, I_d):
  
  f_Ï†(x) = E_Y[Ï†(|Y-x|)(Y-x)] where Y ~ N(0, I_d)

Let's compute this for x = 0 and Ï†(r) = 1/r:

  f_Ï†(0) = E_Y[Y/|Y|] = 0 by symmetry!

This means in the mean-field limit with symmetric distribution,
the drift at the center is zero!

The coercivity in mean-field limit depends on the second moment:
  E[||f_Ï†(X)||Â²] where X ~ Î¼
""")

def mean_field_coercivity(d, n_samples=100000):
    """Compute coercivity in mean-field limit"""
    np.random.seed(42)
    
    # X ~ N(0, I_d), Y ~ N(0, I_d) independent
    X = np.random.randn(n_samples, d)
    Y = np.random.randn(n_samples, d)
    
    r = Y - X
    r_norm = np.linalg.norm(r, axis=1, keepdims=True)
    r_unit = r / (r_norm + 1e-10)
    
    # f_Ï†(X) = E_Y[Ï†(|Y-X|)(Y-X)] â‰ˆ average over Y samples
    # But here we have paired samples, so each X gets one Y
    # For true mean-field, need to average over many Y for each X
    
    # Let's do it properly with more Y samples per X
    n_X = 1000
    n_Y = 1000
    
    X = np.random.randn(n_X, d)
    
    f_phi = np.zeros((n_X, d))
    for i in range(n_X):
        Y = np.random.randn(n_Y, d)
        r = Y - X[i]
        r_norm = np.linalg.norm(r, axis=1, keepdims=True)
        r_unit = r / (r_norm + 1e-10)
        f_phi[i] = np.mean(r_unit, axis=0)
    
    E_f_sq = np.mean(np.sum(f_phi**2, axis=1))
    
    print(f"Mean-field limit, d={d}:")
    print(f"  E[||f_Ï†(X)||Â²] = {E_f_sq:.6f}")
    print(f"  This should be O(1/n_Y) = {1/n_Y:.6f} due to CLT")
    
    return E_f_sq

for d in [1, 2, 3]:
    mean_field_coercivity(d)
    print()

# ============================================================
# SECTION 5: CORRECT INTERPRETATION
# ============================================================
print("\n" + "=" * 70)
print("SECTION 5: CORRECT COERCIVITY INTERPRETATION")
print("=" * 70)

print("""
KEY INSIGHT: The coercivity constant c_H depends on:
1. The hypothesis space H
2. The distribution Ï_T of pairwise distances
3. The number of particles N (finite particle systems)

For FINITE N with iid initial conditions:
- The coercivity c_H scales like (N-1)/NÂ² for large N
- This is because E[||f_Ï†||Â²] ~ (N-1)/NÂ² * E[|Ï†(r)r|Â²]

For the trajectory-based loss (Fei Lu's setup):
- Data comes from trajectories, not just t=0
- The measure Ï_T aggregates over time [0,T]
- Ergodicity of the relative position system is crucial!

The coercivity constants 0.48, 0.87, 0.73 in our theory file
are NOT from Fei Lu's paper - they need to be recomputed
based on the specific setup!
""")

# ============================================================
# SECTION 6: CONVERGENCE RATE
# ============================================================
print("\n" + "=" * 70)
print("SECTION 6: CONVERGENCE RATE VERIFICATION")
print("=" * 70)

print("""
From Fei Lu's paper, the convergence rate is:

Continuous-time observations:
  |||Ï†Ì‚ - Ï†|||Â² â‰² (1/c_HÂ²) Â· (log M / M)^{2s/(2s+1)}

where:
- M = number of trajectories
- s = HÃ¶lder smoothness of Ï†
- c_H = coercivity constant

Discrete-time observations:
  |||Ï†Ì‚ - Ï†||| â‰¤ |||Ï†Ì‚_{âˆ} - Ï†||| + C(âˆš(n/M) + âˆšÎ”t)

where:
- Î”t = observation time gap
- n = dimension of hypothesis space

The rate 2s/(2s+1) is the MINIMAX OPTIMAL rate for 1D nonparametric
regression! This is because we learn Ï†(r)r as a function of r âˆˆ R+.

OUR THEOREM 4 claimed rate n^{-2(s-1)/(2s+d)} which is DIFFERENT!
- Fei Lu: rate in M (number of trajectories)
- Our claim: rate in n (sample size)
- These are measuring different things!
""")

# Let's verify the rate formula
s = sp.Symbol('s', positive=True)
d = sp.Symbol('d', positive=True)

fei_lu_rate = 2*s / (2*s + 1)
our_rate_wrong = 2*(s-1) / (2*s + d)
our_rate_corrected = 2*(s-1) / (2*s + d - 2)
standard_1d_rate = 2*s / (2*s + 1)

print("\nRate comparison (for gradient estimation):")
print(f"  Fei Lu (function estimation):  2s/(2s+1)")
print(f"  Standard 1D regression:        2s/(2s+1)")
print(f"  Our theorem (wrong):           2(s-1)/(2s+d)")
print(f"  Our theorem (corrected):       2(s-1)/(2s+d-2)")

print("\nNumerical comparison (s=2):")
for d_val in [1, 2, 3]:
    print(f"  d={d_val}: Fei Lu = {float(2*2/(2*2+1)):.4f}, " + 
          f"wrong = {float(2*(2-1)/(2*2+d_val)):.4f}, " +
          f"corrected = {float(2*(2-1)/(2*2+d_val-2)):.4f}")

# ============================================================
# SECTION 7: SUMMARY
# ============================================================
print("\n" + "=" * 70)
print("SECTION 7: SUMMARY OF FINDINGS")
print("=" * 70)

print("""
âœ… VERIFIED:
1. System setup and notation match Fei Lu's paper
2. The norm |||Ï†||| = ||Ï†(Â·)Â·||_{LÂ²(Ï_T)} is correct
3. Coercivity ensures identifiability via positive definiteness
4. Convergence rate 2s/(2s+1) is minimax optimal for 1D

âŒ ISSUES IN OUR THEORY:
1. Coercivity constants (0.48, 0.87, 0.73) - source unclear
2. Minimax rate exponent - mismatch between theorem and proof
3. Conditional independence assumption - needs mean-field or t=0

âš ï¸ NEEDS CLARIFICATION:
1. Our loss function vs. Fei Lu's MLE - are they equivalent?
2. Our setup: learning both V and Î¦ vs. Fei Lu: learning only Ï†
3. Trajectory-free vs. trajectory-based learning

ğŸ“ RECOMMENDATIONS:
1. Use Fei Lu's coercivity definition explicitly
2. Clarify the relationship between our loss and MLE
3. Either prove new coercivity bounds or cite properly
""")
