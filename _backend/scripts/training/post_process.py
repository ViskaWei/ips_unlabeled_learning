#!/usr/bin/env python3
"""
è®­ç»ƒåå¤„ç†æ¨¡å— (Post-Processing Module)
=======================================

åŠŸèƒ½ï¼š
- è®­ç»ƒå®Œæˆåè‡ªåŠ¨æ‰§è¡Œçš„å¤„ç†æ­¥éª¤
- ç”Ÿæˆ metrics.csv æ±‡æ€»
- ç”Ÿæˆ summary.json 
- åˆ›å»º exp.md æŠ¥å‘Šéª¨æ¶ï¼ˆä¾› Cursor å¡«å……ï¼‰
- å¤åˆ¶å…³é”®ç»“æœåˆ°çŸ¥è¯†ä¸­å¿ƒ

æ ¸å¿ƒç†å¿µï¼š
- **å‡å°‘ç»™ Cursor çš„ token**ï¼šåªæä¾›ç²¾ç®€çš„ summaryï¼Œè€Œä¸æ˜¯å®Œæ•´æ—¥å¿—
- **è‡ªåŠ¨åŒ–å½’æ¡£**ï¼šç”ŸæˆæŠ¥å‘Šéª¨æ¶ï¼Œä¾¿äºåç»­å¡«å……

è¾“å‡ºæ–‡ä»¶ï¼š
- results/{exp_id}/metrics.csv     - è®­ç»ƒæŒ‡æ ‡æ—¶é—´åºåˆ—
- results/{exp_id}/summary.json    - å®éªŒé…ç½® + æœ€ç»ˆç»“æœ
- results/{exp_id}/report_draft.md - exp.md æŠ¥å‘Šéª¨æ¶

ç”¨æ³•ï¼š
    python post_process.py --exp-id VIT-20251204-xxx --work-dir ~/VIT

ä½œè€…: Viska Wei
æ—¥æœŸ: 2025-12-04
"""

import os
import sys
import json
import csv
import re
from pathlib import Path
from datetime import datetime
from typing import Optional
import argparse


class PostProcessor:
    """è®­ç»ƒåå¤„ç†å™¨"""
    
    # çŸ¥è¯†ä¸­å¿ƒè·¯å¾„
    KNOWLEDGE_CENTER = Path("/home/swei20/Physics_Informed_AI")
    
    def __init__(
        self,
        exp_id: str,
        work_dir: Path,
        results_dir: Optional[Path] = None,
    ):
        self.exp_id = exp_id
        self.work_dir = Path(work_dir)
        self.results_dir = results_dir or (self.work_dir / "results" / exp_id)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # æ—¥å¿—å’Œä¿¡å·ç›®å½•
        self.logs_dir = self.work_dir / "logs"
        self.signals_dir = self.work_dir / "signals"
        
        # è¾“å‡ºæ–‡ä»¶
        self.metrics_csv = self.results_dir / "metrics.csv"
        self.summary_json = self.results_dir / "summary.json"
        self.report_draft = self.results_dir / "report_draft.md"
    
    def _log(self, msg: str, level: str = "INFO"):
        """å¸¦æ—¶é—´æˆ³çš„æ—¥å¿—"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        prefix = {"INFO": "ğŸ“", "SUCCESS": "âœ…", "WARNING": "âš ï¸", "ERROR": "âŒ"}.get(level, "")
        print(f"[{timestamp}] {prefix} {msg}")
    
    def run(self):
        """æ‰§è¡Œæ‰€æœ‰åå¤„ç†æ­¥éª¤"""
        self._log("å¼€å§‹åå¤„ç†...")
        
        # 1. æå–è®­ç»ƒæŒ‡æ ‡
        self.extract_metrics()
        
        # 2. ç”Ÿæˆå®éªŒæ‘˜è¦
        self.generate_summary()
        
        # 3. ç”ŸæˆæŠ¥å‘Šéª¨æ¶
        self.generate_report_draft()
        
        # 4. åŒæ­¥åˆ°çŸ¥è¯†ä¸­å¿ƒï¼ˆå¯é€‰ï¼‰
        # self.sync_to_knowledge_center()
        
        self._log("åå¤„ç†å®Œæˆï¼", "SUCCESS")
        self._print_output_files()
    
    def extract_metrics(self):
        """ä»è®­ç»ƒæ—¥å¿—ä¸­æå–æŒ‡æ ‡åˆ° CSV"""
        log_file = self.logs_dir / f"{self.exp_id}.log"
        
        if not log_file.exists():
            self._log(f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file}", "WARNING")
            return
        
        self._log("æå–è®­ç»ƒæŒ‡æ ‡...")
        
        # æ­£åˆ™æ¨¡å¼ï¼šæ”¯æŒå¤šç§æ—¥å¿—æ ¼å¼
        patterns = {
            "step": re.compile(r'step[=:\s]+(\d+)', re.IGNORECASE),
            "epoch": re.compile(r'epoch[=:\s]+(\d+)', re.IGNORECASE),
            "loss": re.compile(r'(?:train_)?loss[=:\s]+([0-9.eE+-]+)', re.IGNORECASE),
            "val_loss": re.compile(r'val_loss[=:\s]+([0-9.eE+-]+)', re.IGNORECASE),
            "lr": re.compile(r'(?:learning_rate|lr)[=:\s]+([0-9.eE+-]+)', re.IGNORECASE),
            "r2": re.compile(r'r2[=:\s]+([0-9.eE+-]+)', re.IGNORECASE),
            "mae": re.compile(r'mae[=:\s]+([0-9.eE+-]+)', re.IGNORECASE),
        }
        
        rows = []
        current_row = {}
        
        with open(log_file, "r", errors='ignore') as f:
            for line in f:
                for key, pattern in patterns.items():
                    match = pattern.search(line)
                    if match:
                        try:
                            value = float(match.group(1)) if key != "step" and key != "epoch" else int(match.group(1))
                            current_row[key] = value
                        except ValueError:
                            continue
                
                # æ¯æ¬¡æ‰¾åˆ° step æˆ– epoch æ—¶ä¿å­˜ä¸€è¡Œ
                if ("step" in current_row or "epoch" in current_row) and "loss" in current_row:
                    rows.append(current_row.copy())
                    current_row = {}
        
        if not rows:
            self._log("æœªèƒ½æå–åˆ°æŒ‡æ ‡", "WARNING")
            return
        
        # å†™å…¥ CSV
        fieldnames = ["step", "epoch", "loss", "val_loss", "lr", "r2", "mae"]
        with open(self.metrics_csv, "w", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=[fn for fn in fieldnames if any(fn in row for row in rows)])
            writer.writeheader()
            for row in rows:
                writer.writerow({k: v for k, v in row.items() if k in fieldnames})
        
        self._log(f"æå–äº† {len(rows)} æ¡æŒ‡æ ‡è®°å½•", "SUCCESS")
    
    def generate_summary(self):
        """ç”Ÿæˆå®éªŒæ‘˜è¦ JSON"""
        self._log("ç”Ÿæˆå®éªŒæ‘˜è¦...")
        
        summary = {
            "exp_id": self.exp_id,
            "timestamp": datetime.now().isoformat(),
            "status": "completed",
        }
        
        # è¯»å–ä¿¡å·æ–‡ä»¶ä¸­çš„ä¿¡æ¯
        done_file = self.signals_dir / f"{self.exp_id}.done"
        if done_file.exists():
            for line in done_file.read_text().strip().split("\n"):
                if ":" in line:
                    key, value = line.split(":", 1)
                    summary[key.strip()] = value.strip()
        
        # ä» metrics.csv æå–æœ€ç»ˆæŒ‡æ ‡
        if self.metrics_csv.exists():
            with open(self.metrics_csv, "r") as f:
                reader = csv.DictReader(f)
                rows = list(reader)
                if rows:
                    last_row = rows[-1]
                    summary["final_metrics"] = {k: float(v) for k, v in last_row.items() if v}
                    
                    # æœ€ä½³æŒ‡æ ‡
                    if "loss" in last_row:
                        losses = [float(r["loss"]) for r in rows if "loss" in r and r["loss"]]
                        summary["best_loss"] = min(losses)
                        summary["final_loss"] = float(last_row["loss"])
                    
                    if "r2" in last_row:
                        r2s = [float(r["r2"]) for r in rows if "r2" in r and r["r2"]]
                        summary["best_r2"] = max(r2s)
                        summary["final_r2"] = float(last_row["r2"])
        
        # å†™å…¥ JSON
        with open(self.summary_json, "w") as f:
            json.dump(summary, f, indent=2)
        
        self._log("æ‘˜è¦å·²ç”Ÿæˆ", "SUCCESS")
    
    def generate_report_draft(self):
        """ç”Ÿæˆ exp.md æŠ¥å‘Šéª¨æ¶"""
        self._log("ç”ŸæˆæŠ¥å‘Šéª¨æ¶...")
        
        # è¯»å–æ‘˜è¦
        summary = {}
        if self.summary_json.exists():
            with open(self.summary_json) as f:
                summary = json.load(f)
        
        # ä» exp_id æ¨æ–­ä¸»é¢˜
        topic = self._infer_topic(self.exp_id)
        date = datetime.now().strftime("%Y%m%d")
        
        # ç”ŸæˆæŠ¥å‘Šéª¨æ¶
        report = f"""# {self.exp_id} å®éªŒæŠ¥å‘Š

> **å®éªŒ ID**: {self.exp_id}
> **çŠ¶æ€**: ğŸ”„ å¾…å¡«å……
> **æ—¥æœŸ**: {datetime.now().strftime("%Y-%m-%d")}
> **ä½œè€…**: Viska Wei

---

## ğŸ”— ä¸Šæ¸¸è¿½æº¯é“¾æ¥

- **æ¥æºä¼šè¯**: <!-- TODO: é“¾æ¥åˆ° session.md -->
- **é˜Ÿåˆ—å…¥å£**: <!-- TODO: é“¾æ¥åˆ° kanban.md -->

---

## âš¡ æ ¸å¿ƒç»“è®ºé€Ÿè§ˆ

| é¡¹ç›® | å†…å®¹ |
|------|------|
| **ä¸€å¥è¯æ€»ç»“** | <!-- TODO --> |
| **å‡è®¾éªŒè¯** | <!-- âŒ/âœ… H?.? --> |
| **å…³é”®æ•°å­—** | RÂ²={summary.get('final_r2', 'TODO')}, Loss={summary.get('final_loss', 'TODO')} |
| **è®¾è®¡å¯ç¤º** | <!-- TODO --> |

---

## 1. ğŸ¯ ç›®æ ‡

### 1.1 å®éªŒç›®çš„
<!-- TODO: å¡«å†™å®éªŒç›®çš„ -->

### 1.2 é¢„æœŸç»“æœ
<!-- TODO: å¡«å†™é¢„æœŸç»“æœ -->

---

## 2. ğŸ§ª å®éªŒè®¾è®¡

### 2.1 æ•°æ®
<!-- TODO: æ•°æ®é…ç½® -->

### 2.2 æ¨¡å‹ä¸ç®—æ³•
<!-- TODO: æ¨¡å‹é…ç½® -->

### 2.3 è¶…å‚æ•°é…ç½®
| è¶…å‚æ•° | å€¼ |
|--------|-----|
| TODO | TODO |

### 2.4 è¯„ä»·æŒ‡æ ‡
- RÂ²
- MAE
- Loss

---

## 3. ğŸ“Š å®éªŒå›¾è¡¨

<!-- TODO: æ·»åŠ å›¾è¡¨ -->

---

## 4. ğŸ’¡ å…³é”®æ´è§

### 4.1 å®è§‚å±‚æ´è§
<!-- TODO -->

### 4.2 æ¨¡å‹å±‚æ´è§
<!-- TODO -->

### 4.3 å®éªŒå±‚ç»†èŠ‚æ´è§
<!-- TODO -->

---

## 5. ğŸ“ ç»“è®º

### 5.1 æ ¸å¿ƒå‘ç°
<!-- TODO -->

### 5.2 å…³é”®ç»“è®º
<!-- TODO -->

### 5.3 è®¾è®¡å¯ç¤º
<!-- TODO -->

### 5.4 ç‰©ç†è§£é‡Š
<!-- TODO -->

### 5.5 å…³é”®æ•°å­—é€ŸæŸ¥
| æŒ‡æ ‡ | å€¼ |
|------|-----|
| æœ€ç»ˆ RÂ² | {summary.get('final_r2', 'TODO')} |
| æœ€ä½³ RÂ² | {summary.get('best_r2', 'TODO')} |
| æœ€ç»ˆ Loss | {summary.get('final_loss', 'TODO')} |
| æœ€ä½³ Loss | {summary.get('best_loss', 'TODO')} |

### 5.6 ä¸‹ä¸€æ­¥å·¥ä½œ
<!-- TODO -->

---

## 6. ğŸ“ é™„å½•

### 6.1 æ•°å€¼ç»“æœè¡¨
<!-- ä» metrics.csv ç”Ÿæˆ -->

### 6.2 å®éªŒæµç¨‹è®°å½•

**æ‰§è¡Œå‘½ä»¤**:
```bash
# TODO: å¡«å†™æ‰§è¡Œå‘½ä»¤
```

**å…³é”®æ—¥å¿—**:
```
# TODO: ç²˜è´´å…³é”®æ—¥å¿—ç‰‡æ®µ
```

### 6.3 ç›¸å…³æ–‡ä»¶
- æ—¥å¿—: `{self.logs_dir / f"{self.exp_id}.log"}`
- æŒ‡æ ‡: `{self.metrics_csv}`
- æ‘˜è¦: `{self.summary_json}`

---

*æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆäº {datetime.now().isoformat()}*
"""
        
        with open(self.report_draft, "w") as f:
            f.write(report)
        
        self._log("æŠ¥å‘Šéª¨æ¶å·²ç”Ÿæˆ", "SUCCESS")
    
    def _infer_topic(self, exp_id: str) -> str:
        """ä»å®éªŒ ID æ¨æ–­ä¸»é¢˜"""
        lower_id = exp_id.lower()
        
        topics = {
            "cnn": "cnn",
            "conv": "cnn",
            "dilat": "cnn",
            "moe": "moe",
            "expert": "moe",
            "nn": "NN",
            "mlp": "NN",
            "swin": "swin",
            "vit": "swin",
            "transformer": "swin",
            "ridge": "ridge",
            "linear": "ridge",
            "pca": "pca",
            "distill": "distill",
            "latent": "distill",
            "gta": "gta",
            "global": "gta",
            "diffusion": "diffusion",
            "noise": "noise",
            "lightgbm": "lightgbm",
            "lgbm": "lightgbm",
        }
        
        for keyword, topic in topics.items():
            if keyword in lower_id:
                return topic
        
        return "NN"  # é»˜è®¤
    
    def sync_to_knowledge_center(self):
        """åŒæ­¥åˆ°çŸ¥è¯†ä¸­å¿ƒï¼ˆä½¿ç”¨ç»ˆç«¯å‘½ä»¤é¿å…è·¨ä»“åº“å†™å…¥é—®é¢˜ï¼‰"""
        self._log("åŒæ­¥åˆ°çŸ¥è¯†ä¸­å¿ƒ...")
        
        topic = self._infer_topic(self.exp_id)
        target_dir = self.KNOWLEDGE_CENTER / "logg" / topic
        
        # ä½¿ç”¨ç»ˆç«¯å‘½ä»¤å¤åˆ¶
        import subprocess
        
        # å¤åˆ¶ summary.json
        if self.summary_json.exists():
            subprocess.run([
                "cp", str(self.summary_json),
                str(target_dir / f"{self.exp_id}_summary.json")
            ], check=False)
        
        # å¤åˆ¶æŠ¥å‘Šéª¨æ¶
        if self.report_draft.exists():
            date = datetime.now().strftime("%Y%m%d")
            subprocess.run([
                "cp", str(self.report_draft),
                str(target_dir / f"exp_{self.exp_id}_{date}.md")
            ], check=False)
        
        self._log("åŒæ­¥å®Œæˆ", "SUCCESS")
    
    def _print_output_files(self):
        """æ‰“å°è¾“å‡ºæ–‡ä»¶åˆ—è¡¨"""
        print()
        print("â•" * 50)
        print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
        print("â•" * 50)
        
        files = [
            (self.metrics_csv, "è®­ç»ƒæŒ‡æ ‡ CSV"),
            (self.summary_json, "å®éªŒæ‘˜è¦ JSON"),
            (self.report_draft, "æŠ¥å‘Šéª¨æ¶ MD"),
        ]
        
        for path, desc in files:
            if path.exists():
                size = path.stat().st_size
                print(f"  âœ… {path.name:<25} ({size:,} bytes) - {desc}")
            else:
                print(f"  âšª {path.name:<25} (æœªç”Ÿæˆ) - {desc}")
        
        print()
        print("ğŸ’¡ æç¤º:")
        print(f"   1. æŸ¥çœ‹æ‘˜è¦: cat {self.summary_json}")
        print(f"   2. å¡«å……æŠ¥å‘Š: æŠŠ {self.report_draft} å†…å®¹ç»™ Cursor")
        print(f"   3. å½’æ¡£: a {self.exp_id}")
        print("â•" * 50)


def generate_cursor_prompt(summary_json: Path, metrics_csv: Path) -> str:
    """
    ç”Ÿæˆç»™ Cursor çš„ç²¾ç®€ prompt
    
    è¿™æ˜¯æ ¸å¿ƒå‡½æ•°ï¼šä¸æŠŠæ•´ä¸ªæ—¥å¿—ç»™ Cursorï¼Œåªç»™æ‘˜è¦
    """
    prompt_parts = []
    
    # è¯»å–æ‘˜è¦
    if summary_json.exists():
        with open(summary_json) as f:
            summary = json.load(f)
        
        prompt_parts.append("## å®éªŒæ‘˜è¦\n")
        prompt_parts.append(f"- å®éªŒ ID: {summary.get('exp_id', 'unknown')}")
        prompt_parts.append(f"- çŠ¶æ€: {summary.get('status', 'unknown')}")
        
        if "final_metrics" in summary:
            prompt_parts.append("\n### æœ€ç»ˆæŒ‡æ ‡")
            for k, v in summary["final_metrics"].items():
                prompt_parts.append(f"- {k}: {v}")
        
        if "best_r2" in summary:
            prompt_parts.append(f"\n### å…³é”®æ•°å­—")
            prompt_parts.append(f"- æœ€ä½³ RÂ²: {summary['best_r2']}")
            prompt_parts.append(f"- æœ€ç»ˆ RÂ²: {summary.get('final_r2', 'N/A')}")
            prompt_parts.append(f"- æœ€ä½³ Loss: {summary.get('best_loss', 'N/A')}")
            prompt_parts.append(f"- æœ€ç»ˆ Loss: {summary.get('final_loss', 'N/A')}")
    
    # è¯»å–æœ€åå‡ è¡ŒæŒ‡æ ‡
    if metrics_csv.exists():
        prompt_parts.append("\n### æœ€å 5 ä¸ªæ•°æ®ç‚¹")
        with open(metrics_csv) as f:
            reader = csv.DictReader(f)
            rows = list(reader)[-5:]
            if rows:
                headers = rows[0].keys()
                prompt_parts.append("| " + " | ".join(headers) + " |")
                prompt_parts.append("| " + " | ".join(["---"] * len(headers)) + " |")
                for row in rows:
                    prompt_parts.append("| " + " | ".join(str(row.get(h, "")) for h in headers) + " |")
    
    prompt_parts.append("\n---")
    prompt_parts.append("è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œå¸®æˆ‘ï¼š")
    prompt_parts.append("1. æ€»ç»“æ ¸å¿ƒç»“è®ºï¼ˆä¸€å¥è¯ï¼‰")
    prompt_parts.append("2. æç‚¼å…³é”®æ´è§")
    prompt_parts.append("3. ç»™å‡ºè®¾è®¡å»ºè®®")
    prompt_parts.append("4. å»ºè®®ä¸‹ä¸€æ­¥å®éªŒ")
    
    return "\n".join(prompt_parts)


def parse_args():
    parser = argparse.ArgumentParser(description="è®­ç»ƒåå¤„ç†")
    parser.add_argument("--exp-id", "-e", required=True, help="å®éªŒ ID")
    parser.add_argument("--work-dir", "-w", default=os.getcwd(), help="å·¥ä½œç›®å½•")
    parser.add_argument("--generate-prompt", action="store_true", help="ç”Ÿæˆ Cursor prompt")
    return parser.parse_args()


def main():
    args = parse_args()
    
    processor = PostProcessor(
        exp_id=args.exp_id,
        work_dir=Path(args.work_dir),
    )
    
    processor.run()
    
    if args.generate_prompt:
        print("\n" + "=" * 50)
        print("ğŸ“‹ Cursor Prompt (å¤åˆ¶ä»¥ä¸‹å†…å®¹):")
        print("=" * 50)
        prompt = generate_cursor_prompt(processor.summary_json, processor.metrics_csv)
        print(prompt)


if __name__ == "__main__":
    main()

